\documentclass[12pt,oneside,a4paper]{book}
\usepackage{fullpage}
%\usepackage[toc,page]{appendix}

\usepackage{caption}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{amsthm}

% Quotes
\usepackage{upquote}

% Code listing setup
\usepackage{listings}
\usepackage{color} %need to use color to define those colors below
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\lstset{
    commentstyle=\color{red},
    keywordstyle=\color{blue},
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    literate={~} {$\sim$}{1},
    escapechar=Â£,
    basicstyle=\scriptsize,
    frame=single,
    showstringspaces=false,
    breaklines,
    language=Octave,
}
% Rermove indent in list of listing
\makeatletter
\renewcommand*{\l@lstlisting}{\@dottedtocline{1}{0em}{2.3em}}
\makeatother


% Hyper link setup
\usepackage{hyperref}
\hypersetup
{
    colorlinks=true,
    linkcolor=blue,
    urlcolor=red,
    linktoc=all
}

% Graphicx setup
\usepackage{graphicx}
\graphicspath{ {images/} }

% No indent at the first line of a paragraph
\setlength\parindent{0pt}

% List of equations
\usepackage[titles]{tocloft}  % THIS HELPS US TO CUSTOMIZE OUT LISTS

\newcommand{\listequationsname}{List of Equations}
\newlistof{myequations}{equ}{\listequationsname}
% myequation takes in its display name
\newcommand{\myequations}[1]{%
% display name is printed
%\addcontentsline{equ}{myequations}{\protect\numberline{\theequation}#1}\par}
\addcontentsline{equ}{myequations}{\protect\numberline{\theequation}#1}}
\setlength{\cftmyequationsnumwidth}{2.3em}% Width of equation number in List of Equations

% List of alignment
\newcommand{\listalignsname}{List of Formulas}
\newlistof{myaligns}{ali}{\listalignsname}
% myaligns takes in its display name
\newcommand{\myaligns}[1]{%
% display name is printed
%\refstepcounter{myaligns}  % THIS LINE IS ONLY FOR REVIEW PURPOSE
\addcontentsline{ali}{myaligns}{\protect\numberline{\theequation}#1}}
\setlength{\cftmyalignsnumwidth}{2.3em}% Width of formula number in List of Formulas



%\usepackage{lipsum}
%
%\newtheoremstyle{break}
%{\topsep}{\topsep}%
%{\itshape}{}%
%{\bfseries}{\bfseries}%
%{\newline}{}%
%\theoremstyle{break}

\newtheoremstyle{break}% name
{}%         Space above, empty = `usual value'
{}%         Space below
{\itshape}% Body font
{}%         Indent amount (empty = no indent, \parindent = para indent)
{\bfseries}% Thm head font
{\bfseries}%        Punctuation after thm head
{\newline}% Space after thm head: \newline = linebreak
{}%         Thm head spec

\theoremstyle{break}

\newtheorem{defi}{Definition}

% List of Definition
\newcommand{\listdefsname}{List of Definitions} %define the name that appears as a title of our list

\newlistof{mydefs}{def}{\listdefsname}
\newcommand{\mydefs}[1]{%
\refstepcounter{mydefs}
\addcontentsline{def}{mydefs}{\protect\numberline{\thechapter.\themydefs}#1}}
\setlength{\cftmydefsnumwidth}{2.3em}


\newtheorem{theorem}{Theorem}



\setlength{\cftfigindent}{0pt}  % remove indentation from figures in lof
\setlength{\cfttabindent}{0pt}  % remove indentation from tables in lot


% INDICES: NOT SO USEFUL IN MY CASE
% \usepackage{imakeidx}
% \makeindex[columns=2, title=Alphabetical Index, intoc]


% Glossary
\newcommand*{\glossaryname}{Glossary}
\usepackage[nonumberlist,toc]{glossaries}
\newcommand{\dictentry}[2]{%
  \newglossaryentry{#1}{name=#1,description={#2}}%
  \glslink{#1}{}%
}
% \makeglossaries


% To use mathbb represents mathematical form of capital letters
\usepackage{amsfonts} 

%% ---------------------------- BEGIN DOCUMENT ----------------------------

\begin{document}
\boldmath
\begin{titlepage}
    \begin{center}
        %\vspace*{1cm}
        
        \Huge
        \textbf{}
        
        \vspace{0.5cm}
        \LARGE
        \textbf{Implementation of Word2Vec using Tensorflow}
        
        \vspace{1.5cm}
        
        \textbf{Hong Anh VU}\\
        vuhonganh91 at gmail dot com\\
        May 2017
        
        \vfill        

        \begin{figure}[!ht]
          \centering
          \includegraphics[scale=0.28]{xLogo.eps}
        \end{figure}
        
        \Large
        Ecole Polytechnique\\
        Promotion X2011\\
        
    \end{center}
\end{titlepage}



%\setcounter{tocdepth}{2}
% \tableofcontents

% \makeatletter
% \def\@@underline#1{#1}
% \tableofcontents
% \makeatother


% \listoffigures

%put in comment because there is no code yet
%\lstlistoflistings  

%\listofmyequations %use myaligns instead

% \listofmyaligns

% \listofmydefs

% \dictentry{PGM}{Probabilistic Graphical Model}%
% \dictentry{PDF}{Probability Distribution Function}%
% \dictentry{CPD}{Conditional Probability Distribution}%
% \dictentry{BN}{Bayesian Network}%
% \dictentry{2TBN}{2-time-slice Bayesian Network}%
% \dictentry{DBN}{Dynamic Bayesian Network}%
% \printglossary[style=list]%

%\chapter{Week 1} 
%\input{week1/note}
In Word2Vec model, we slide a window through the text corpus, and at each time we
\begin{itemize}
    \item either try to predict the center word given the surrounding words in that window (CBOW)
    \item or predict surrounding words given the center word (Skip-gram).
\end{itemize}
In each window, the given words are also called input word, and the others are called output word. 

Note that the word2vec model is based on these conditioned probabilities to achieve a vector representation of words that minimize the penalty of wrong predictions. 
\section{Skip-gram model}
\subsection{Softmax Cross-Entropy Loss}
Denote $U$, $V$ the output and input embedding matrix respectively. In a window, denote center word by its index $c$ and one output word by its index $o$. The prediction of $o$ is made using softmax function:
\begin{align}
\label{f1}
\hat{y}_o = p(o | c) = \frac{exp(u_o^T v_c)}{\sum_{w=1}^W exp(u_w^T v_c)} 
\end{align}
where $W$ is the vocabulary size of the corpus. $\hat{y}_o$ is a vector where each element $\hat{y}_{o_i}$ represents the probability the word at that index $i$ is the correct word. Denote the ground truth vector $y$ which is one hot representation of the word $o$. We can define a cross entropy loss function as below:
\begin{align}
J_{SCE} = CE(y, \hat{y}) = -\sum_i y_i \ln(\hat{y}_i) = - y_o \ln(\hat{y}_o)
\end{align}

The downside of softmax cross entropy loss is the expensive computation when we have to compute scalar products of $v_c$ to the whole vocabulary in formula \ref{f1}. That's why we will not use this in practice. We use instead the negative sampling scheme described in the next subsection.
\subsection{Negative Sampling Loss}
Keeping notations defined as above, we assume particularly $K$ the number of negative samples (word) are drawn and $o \notin \{1,...,K\}$. The negative sampling loss is defined as:
\begin{align}
J_{NEG} = -\ln(\sigma(u_o^T v_c)) - \sum_{k=1}^K \ln(\sigma(-u_k^T v_c))
\end{align}
The idea is: if $u_o$ is the word that often goes with $v_c$ then its similarity is high. A measure for that is just a scalar product of two vectors. Also, when K samples are drawn, they have small chance to be close to $v_c$ given a big vocabulary. The distribution to draw sample is $Unigram^{3/4}$ to reduce the sampling of frequent words. However we will use the NCE loss function provided by Tensorflow library and hence not to due with this sampling step. At test time, I found that some frequent words really affect the results. Maybe we need to filter out these frequent words and punctuation (dot, comma, etc.).

% \chapter{Week 2}
% \input{week2/note}

% \chapter{Week 3}
% \input{week3/note}

% \chapter{Personal Note}
% \input{personal/note}

%\printindex
\end{document}
